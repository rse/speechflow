##
##  sample.yaml -- Speechflow Sample Audio Processing Graphs
##

#  capture audio from microphone to file
capture-microphone1: |
    device(device: "coreaudio:Elgato Wave:3", mode: "r") |
    file(path: "capture.pcm", mode: "w", type: "audio")
capture-microphone: |
    device(device: "wasapi:VoiceMeeter Output", mode: "r") |
    file(path: "capture.pcm", mode: "w", type: "audio")

#  generate audio file with narration of text file
generate-narration: |
    file(path: argv.0, mode: "r", type: "audio") |
    deepgram(key: env.SPEECHFLOW_KEY_DEEPGRAM)   |
    file(path: argv.1, mode: "w", type: "text")

#  pass-through audio from microphone to speaker and in parallel record it to file
microphone-to-speaker: |
    device(device: "wasapi:VoiceMeeter Output", mode: "r") | {
        file(path: "capture.pcm", mode: "w", type: "audio"),
        device(device: "wasapi:VoiceMeeter VAIO3 Input", mode: "w")
    }

#  translate stdin to stdout
translation: |
    file(path: "-", mode: "r", type: "text") |
    deepl(key: env.SPEECHFLOW_KEY_DEEPL, src: "de", dst: "en-US") |
    file(path: "-", mode: "w", type: "text")

#  sample for development
sample4: |
    file(path: "sample.txt", mode: "r", type: "text") |
    opus(src: "de", dst: "en") |
    file(path: "out.txt", mode: "w", type: "text")
sample: |
    device(device: "coreaudio:Elgato Wave:3", mode: "r") |
    trace(type: "audio", name: "audio") |
    whisper(language: "de", model: "v1-small") |
    trace(type: "text", name: "text") |
    file(path: "sample.txt", mode: "w", type: "text")
# trace(type: "text", name: "text") |
# wav(mode: "encode") | file(path: "sample.wav", mode: "w", type: "audio"),
# trace(type: "audio", name: "audio")
# } |
sample1: |
    device(device: "coreaudio:Elgato Wave:3", mode: "r") |
    trace(type: "audio", name: "audio1") |
    deepgram() |
    trace(type: "text", name: "text1") |
    gemma(src: "de", dst: "en") |
    trace(type: "text", name: "text2") |
    elevenlabs(voice: "Mark", speed: 1.05) |
    trace(type: "audio", name: "audio2") |
    ffmpeg(dst: "wav") |
    trace(type: "audio", name: "audio3") |
    file(path: "sample.wav", mode: "w", type: "audio")
sample2: |
    device(device: "coreaudio:Elgato Wave:3", mode: "r") |
    deepgram() |
    ffmpeg(dst: "wav") |
    file(path: "sample.wav", mode: "w", type: "audio")
sample3: |
    file(path: "sample.txt", mode: "r", type: "text") |
    elevenlabs(voice: "Mark", speed: 1.05) |
    ffmpeg(dst: "wav") |
    file(path: "sample.wav", mode: "w", type: "audio")

filmstudio: |
    device(device: "wasapi:VoiceMeeter A3", mode: "r") | {
        wav(mode: "encode") |
            file(path: "program-de.wav", mode: "w", type: "audio"),
        deepgram(key: env.SPEECHFLOW_KEY_DEEPGRAM, language: "de") | {
            file(path: "program-de.txt", mode: "w", type: "text"),
            deepl(key: env.SPEECHFLOW_KEY_DEEPL, src: "de", dst: "en-US") | {
                file(path: "program-en.txt", mode: "w", type: "text"),
                subtitle(format: "vtt") | {
                    file(path: "program-en.vtt", mode: "w", type: "text"),
                    mqtt(url: "mqtt://127.0.0.1:1883",
                        username: env.SPEECHFLOW_MQTT_USER,
                        password: env.SPEECHFLOW_MQTT_PASS,
                        topic: "stream/studio/sender")
                },
                subtitle(format: "srt") |
                    file(path: "program-en.srt", mode: "w", type: "text"),
                elevenlabs(voice: "Mark", speed: 1.05, language: "en") | {
                    wav(mode: "encode") |
                        file(path: "program-en.wav", mode: "w", type: "audio"),
                    device(device: "wasapi:VoiceMeeter IN2", mode: "w")
                }
            }
        }
    }

